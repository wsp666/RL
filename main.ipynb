{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DQN to Play Crowdsourcing distribution\n",
    "\n",
    "PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aab43673d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 创造环境，打印环境对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Myenv:\n",
    "    def __init__(self):\n",
    "        # 定义属性\n",
    "        self.state_dim =  40                                # 状态空间的维度\n",
    "        self.action_dim =  30                               # 动作空间的维度\n",
    "        self._max_episode_steps = 1000                    # 每一回合最大迭代数\n",
    "        pass\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 定义动作转跳\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # 环境重置\n",
    "        pass\n",
    "    \n",
    "    # def render(self):\n",
    "    #     # 绘制图像\n",
    "    #     pass\n",
    "    \n",
    "    # def close(self):\n",
    "    #     # 关闭窗口\n",
    "    #     pass\n",
    "\n",
    "\n",
    "env = Myenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNReplayer:\n",
    "    def __init__(self, capacity):\n",
    "        # 记忆池用DataFrame格式储存\n",
    "        self.memory = pd.DataFrame(index=range(capacity),\n",
    "                columns=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.i = 0\n",
    "        self.count = 0\n",
    "        self.capacity = capacity                # 记忆池的容量\n",
    "\n",
    "    # 储存经验，超过容量重新填充\n",
    "    def store(self, *args):\n",
    "        self.memory.loc[self.i] = args\n",
    "        self.i = (self.i + 1) % self.capacity                       \n",
    "        self.count = min(self.count + 1, self.capacity)\n",
    "\n",
    "    # 经验抽取\n",
    "    def sample(self, size):\n",
    "        indices = np.random.choice(self.count, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\n",
    "                self.memory.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义DQN代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.action_n = env.action_dim                                              # 来自环境的动作数\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # 容量capacity定义10000\n",
    "        self.replayer = DQNReplayer(10000)\n",
    "\n",
    "        ### 输入状态得到输出动作------评价网络\n",
    "        self.evaluate_net = self.build_net(\n",
    "                input_size=env.state_dim,\n",
    "                hidden_sizes=[64, 64], output_size=self.action_n)                       # 来自环境的状态数\n",
    "        self.optimizer = optim.Adam(self.evaluate_net.parameters(), lr=0.001)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        ### 输入状态得到输出动作------目标网络\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip(\n",
    "                [input_size,] + hidden_sizes, hidden_sizes + [output_size,]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def reset(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory = []\n",
    "            self.target_net = copy.deepcopy(self.evaluate_net)\n",
    "\n",
    "    # agent决策\n",
    "    def step(self, observation, reward, done):\n",
    "        if self.mode == 'train' and np.random.rand() < 0.001:\n",
    "            # epsilon-greedy policy in train mode\n",
    "            action = np.random.randint(self.action_n)\n",
    "        else:\n",
    "            state_tensor = torch.as_tensor(observation,\n",
    "                    dtype=torch.float).squeeze(0)\n",
    "            q_tensor = self.evaluate_net(state_tensor)\n",
    "            action_tensor = torch.argmax(q_tensor)                  # 选择最大的Q值对应的动作\n",
    "            action = action_tensor.item()                           # 选择最大的Q值\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory += [observation, reward, done, action]      # 记录当前的state、reward、done、action\n",
    "            if len(self.trajectory) >= 8:\n",
    "                state, _, _, act, next_state, reward, done, _ = \\\n",
    "                        self.trajectory[-8:]\n",
    "                self.replayer.store(state, act, reward, next_state, done)\n",
    "            if self.replayer.count >= self.replayer.capacity * 0.95:\n",
    "                    # skip first few episodes for speed\n",
    "                self.learn()                                                # 开始学习\n",
    "        return action\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        # replay\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "                self.replayer.sample(1024) # replay transitions\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\n",
    "\n",
    "        # train\n",
    "        next_q_tensor = self.target_net(next_state_tensor)\n",
    "        next_max_q_tensor, _ = next_q_tensor.max(axis=-1)\n",
    "        target_tensor = reward_tensor + self.gamma * (1. - done_tensor) * next_max_q_tensor\n",
    "        pred_tensor = self.evaluate_net(state_tensor)\n",
    "        q_tensor = pred_tensor.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "        loss_tensor = self.loss(target_tensor, q_tensor)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_tensor.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "agent = DQNAgent(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟一个回合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:56:31 [INFO] ==== train ====\n"
     ]
    }
   ],
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\n",
    "    observation, reward, done = env.reset(), 0., False                                  ## 来自环境的reset函数\n",
    "    agent.reset(mode=mode)\n",
    "    episode_reward, elapsed_steps = 0., 0\n",
    "    while True:\n",
    "        action = agent.step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()                                                               ## 来自环境的render函数\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)                                ## 来自环境的step函数\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\n",
    "            break\n",
    "    agent.close()\n",
    "    return episode_reward, elapsed_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "model=agent.evaluate_net\n",
    "y=model(torch.Tensor(np.random.random(env.state_dim)))\n",
    "g = make_dot(y)\n",
    "g.render('espnet_model', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强化学习训练Q表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============ train ============')\n",
    "episode_rewards = []\n",
    "for episode in range(10000):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent,\n",
    "            max_episode_steps=env._max_episode_steps, mode='train')                  ## 来自环境的_max_episode_steps参数\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('train episode %d: reward = %.2f, steps = %d'%\n",
    "            (episode, episode_reward, elapsed_steps))\n",
    "    if np.mean(episode_rewards[-10:]) > -110:                                        ## 如果后期稳定到好的效果就跳出循环\n",
    "        break\n",
    "plt.plot(episode_rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回合测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('============ test ============')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print('test episode %d: reward = %.2f, steps = %d'%\n",
    "            (episode, episode_reward, elapsed_steps))\n",
    "print('average episode reward = %.2f ± %.2f'%\n",
    "        (np.mean(episode_rewards), np.std(episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
